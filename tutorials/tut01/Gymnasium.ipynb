{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gymnasium\n",
        "\n",
        "Gymnasium is a project that provides an API for all single agent reinforcement learning environments. We will outline the basics of how to use Gymnasium including its four key functions: `make`, `Env.reset`, `Env.step` and `Env.render`.\n",
        "\n",
        "At the core of Gymnasium is `Env`, a high-level python class representing a markov decision process (MDP). The class provides users the ability generate an initial state, transition / move to new states given an action and the visualise the environment. Alongside `Env`, `Wrapper` are provided to help augment / modify the environment, in particular, the agent observations, rewards and actions taken."
      ],
      "metadata": {
        "id": "yR6-O5_yjanB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install swig\n",
        "!pip install \"gymnasium[all]\""
      ],
      "metadata": {
        "id": "K7rBXmawosGs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Use"
      ],
      "metadata": {
        "id": "mzDIGVLEmvrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing environments is very easy in Gymnasium and can be done via the `make` function:"
      ],
      "metadata": {
        "id": "xPTozTyakDT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "HF1tIpWKjJby"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will return an `Env` for users to interact with.\n"
      ],
      "metadata": {
        "id": "HSmu23xDkLmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classic “agent-environment loop” pictured below is simplified representation of reinforcement learning that Gymnasium implements."
      ],
      "metadata": {
        "id": "VE4rcnCyH1EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://gymnasium.farama.org/_images/AE_loop.png' />\n",
        "</center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "2NTVrXKDlCRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical workflow\n",
        "\n",
        "First, an environment is created using `make` with an additional keyword ``\"render_mode\"`` that specifies how the environment should be visualised.\n",
        "\n",
        "After initializing the environment, we `Env.reset` the environment to get the first observation of the environment along with an additional information. For initializing the environment with a particular random seed or options (see the environment documentation for possible values) use the ``seed`` or ``options`` parameters with `reset`.\n",
        "\n",
        "As we wish to continue the agent-environment loop until the environment ends, which is in an unknown number of timesteps, we define ``episode_over`` as a variable to know when to stop interacting with the environment along with a while loop that uses it.\n",
        "\n",
        "Next, the agent performs an action in the environment, `Env.step` executes the select actions to update the environment. As a result, the agent receives a new observation from the updated environment along with a reward for taking the action. One such action-observation exchange is referred to as a **timestep**.\n",
        "\n",
        "However, after some timesteps, the environment may end, this is called the terminal state. In gymnasium, if the environment has terminated, this is returned by `step` as the third variable, ``terminated``.\n",
        "Similarly, we may also want the environment to end after a fixed number of timesteps, in this case, the environment issues a truncated signal. If either of ``terminated`` or ``truncated`` are ``True`` then we end the episode but in most cases users might wish to restart the environment, this can be done with `env.reset()`."
      ],
      "metadata": {
        "id": "pO6IWQ2BmVqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(10):\n",
        "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Q5RGtq0upkm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88aa65cb-15aa-495b-955a-a6e6508851f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action and observation spaces"
      ],
      "metadata": {
        "id": "DhnxbqdKm1rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every environment specifies the format of valid actions and observations with the `action_space` and `observation_space` attributes. This is helpful for both knowing the expected input and output of the environment as all valid actions and observation should be contained with their respective space.\n",
        "\n",
        "Importantly, `Env.action_space` and `Env.observation_space` are instances of `Space`, a high-level python class that provides the key functions: `Space.contains` and `Space.sample`. Gymnasium has support for a wide range of spaces that users might need:\n",
        "\n",
        "\n",
        "\n",
        "- `Box`: describes bounded space with upper and lower limits of any n-dimensional shape.\n",
        "- `Discrete`: describes a discrete space where ``{0, 1, ..., n-1}`` are the possible values our observation or action can take.\n",
        "- `MultiBinary`: describes a binary space of any n-dimensional shape.\n",
        "- `MultiDiscrete`: consists of a series of `Discrete` action spaces with a different number of actions in each element.\n",
        "- `Text`: describes a string space with a minimum and maximum length\n",
        "- `Dict`: describes a dictionary of simpler spaces.\n",
        "- `Tuple`: describes a tuple of simple spaces.\n",
        "- `Graph`: describes a mathematical graph (network) with interlinking nodes and edges\n",
        "- `Sequence`: describes a variable length of simpler space elements.\n"
      ],
      "metadata": {
        "id": "WIBtTIPTJXe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.spaces import Box, Discrete, Tuple\n",
        "import numpy as np\n",
        "\n",
        "Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwqq3GiRLaXp",
        "outputId": "d94d039a-d838-4f75-9d05-399300619fbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box([-1. -2.], [2. 4.], (2,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation_space = Discrete(3, start=-1, seed=42)  # {-1, 0, 1}\n",
        "observation_space.sample() # Generates a single random sample from this space."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47jVt_8PcAO",
        "outputId": "f7eba131-a78b-4487-c405-f12df487f1e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation_space = Tuple((Discrete(2), Box(-1, 1, shape=(2,))), seed=42)\n",
        "observation_space.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Cs62vNPb3f",
        "outputId": "53988dbc-ce0d-4363-9bea-415079b6da4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, array([-0.3991573 ,  0.21649833], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more example usage of spaces, see their [documentation](https://gymnasium.farama.org/api/spaces/) along with utility functions."
      ],
      "metadata": {
        "id": "uEVY86dDRPgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modifying the environment"
      ],
      "metadata": {
        "id": "9TZpduZPm8nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrappers are a convenient way to modify an existing environment without having to alter the underlying code directly.\n",
        "\n",
        "Gymnasium already provides many commonly used wrappers. Some examples:\n",
        "\n",
        "- `TimeLimit`: Issues a truncated signal if a maximum number of timesteps has been exceeded (or the base environment has issued a truncated signal).\n",
        "- `ClipAction`: Clips any action passed to ``step`` such that it lies in the base environment's action space.\n",
        "- `RescaleAction`: Applies an affine transformation to the action to linearly scale for a new low and high bound on the environment.\n",
        "- `TimeAwareObservation`: Add information about the index of timestep to observation.\n",
        "\n",
        "Most environments that are generated via `gymnasium.make` will already be wrapped by default using the `TimeLimit`, `OrderEnforcing` and `PassiveEnvChecker`.\n",
        "\n",
        "In order to wrap an environment, you must first initialize a base environment. Then you can pass this environment along with (possibly optional) parameters to the wrapper's constructor:"
      ],
      "metadata": {
        "id": "O_ApNYszR5Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import FlattenObservation, RescaleAction, TimeAwareObservation\n",
        "base_env = gym.make(\"CarRacing-v2\")\n",
        "base_env.action_space\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPwZVcNSTAIg",
        "outputId": "b1cf6b81-57ed-4dac-aaca-4094ad8c6bcd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box([-1.  0.  0.], 1.0, (3,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_env = RescaleAction(base_env, min_action=0, max_action=1)\n",
        "wrapped_env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYiU7dMKa2ed",
        "outputId": "f911f31c-f378-48f9-cca8-c14895a1d3ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0.0, 1.0, (3,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_env = gym.make(\"CartPole-v1\")\n",
        "base_env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1M0LjD8c-gp",
        "outputId": "98c8eb27-7b44-4024-b1ad-6942bdbef7ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.01136441,  0.00864498, -0.03800799,  0.00320334], dtype=float32),\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = TimeAwareObservation(base_env)\n",
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdEfchBIcjNL",
        "outputId": "c54a079c-fb2c-4df6-d823-46e413cda3b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.0442987 , -0.04051058,  0.03965355,  0.03041063,  0.        ]), {})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(env.action_space.sample())[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKBVbsXKdgc9",
        "outputId": "92e828e2-d2ec-4683-ecca-807b5ef3edee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.04510892, -0.23617809,  0.04026176,  0.33533627,  1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CarRacing-v2\")\n",
        "env.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSiR4H70drhs",
        "outputId": "397fcc9a-e4af-4b60-822d-c5594949eda6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96, 96, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = FlattenObservation(env)\n",
        "env.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsbIyVywdvrw",
        "outputId": "fc2dbda2-99ef-4dec-aa2d-0caa627ac647"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27648,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access the environment underneath the **first** wrapper by using the `gymnasium.Wrapper.env` attribute. As the `gymnasium.Wrapper` class inherits from `gymnasium.Env` then `gymnasium.Wrapper.env` can be another wrapper."
      ],
      "metadata": {
        "id": "5iJv4Xm8bNJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_env.env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUXfk4LDbZZP",
        "outputId": "48d0f4a9-19e2-49eb-e4aa-90619225a11b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CarRacing<CarRacing-v2>>>>>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to get to the environment underneath **all** of the layers of wrappers, you can use the `gymnasium.Wrapper.unwrapped` attribute."
      ],
      "metadata": {
        "id": "FTkeqYCAboEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_env.unwrapped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrJUOvI9bsr6",
        "outputId": "f6c82251-48f6-449a-9fe7-8fa6f2d7b1e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gymnasium.envs.box2d.car_racing.CarRacing at 0x7ee0e6da31c0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three common things you might want a wrapper to do:\n",
        "\n",
        "- Transform actions before applying them to the base environment\n",
        "\n",
        "- Transform observations that are returned by the base environment\n",
        "\n",
        "- Transform rewards that are returned by the base environment\n",
        "\n",
        "Such wrappers can be easily implemented by inheriting from `gymnasium.ActionWrapper`,` gymnasium.ObservationWrapper`, or `gymnasium.RewardWrapper` and implementing the respective transformation. If you need a wrapper to do more complicated tasks, you can inherit from the `gymnasium.Wrapper` class directly.\n"
      ],
      "metadata": {
        "id": "2sY9jeGNcN3s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIIJbSyQW9R-"
      },
      "source": [
        "### Example: normalize actions\n",
        "\n",
        "It is usually a good idea to normalize observations and actions before giving it to the agent, this prevents this [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n",
        "\n",
        "In this example, we are going to normalize the action space of *Pendulum-v1* so it lies in [-1, 1] instead of [-2, 2].\n",
        "\n",
        "Note: here we are dealing with continuous actions, hence the `gym.Box` space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F5E6kZfzW8vy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class NormalizeActionWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    :param env: (gym.Env) Gym environment that will be wrapped\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        # Retrieve the action space\n",
        "        action_space = env.action_space\n",
        "        assert isinstance(\n",
        "            action_space, gym.spaces.Box\n",
        "        ), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
        "        # Retrieve the max/min values\n",
        "        self.low, self.high = action_space.low, action_space.high\n",
        "\n",
        "        # We modify the action space, so all actions will lie in [-1, 1]\n",
        "        env.action_space = gym.spaces.Box(\n",
        "            low=-1, high=1, shape=action_space.shape, dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Call the parent constructor, so we can access self.env later\n",
        "        super(NormalizeActionWrapper, self).__init__(env)\n",
        "\n",
        "    def rescale_action(self, scaled_action):\n",
        "        \"\"\"\n",
        "        Rescale the action from [-1, 1] to [low, high]\n",
        "        (no need for symmetric action space)\n",
        "        :param scaled_action: (np.ndarray)\n",
        "        :return: (np.ndarray)\n",
        "        \"\"\"\n",
        "        return self.low + (0.5 * (scaled_action + 1.0) * (self.high - self.low))\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Reset the environment\n",
        "        \"\"\"\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        :param action: ([float] or int) Action taken by the agent\n",
        "        :return: (np.ndarray, float,bool, bool, dict) observation, reward, final state? truncated?, additional informations\n",
        "        \"\"\"\n",
        "        # Rescale action from [-1, 1] to original [low, high] interval\n",
        "        rescaled_action = self.rescale_action(action)\n",
        "        obs, reward, terminated, truncated, info = self.env.step(rescaled_action)\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJ0eahNaR6K"
      },
      "source": [
        "#### Test before rescaling actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UEnjBwisaQIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff5a0f2-7187-4389-8018-df917aec646a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.]\n",
            "[-1.5651957]\n",
            "[1.0477383]\n",
            "[1.0658668]\n",
            "[-1.2885776]\n",
            "[-0.5330492]\n",
            "[-0.80838174]\n",
            "[0.21973506]\n",
            "[0.10782841]\n",
            "[0.5231576]\n",
            "[-0.29341736]\n"
          ]
        }
      ],
      "source": [
        "original_env = gym.make(\"Pendulum-v1\")\n",
        "\n",
        "print(original_env.action_space.low)\n",
        "for _ in range(10):\n",
        "    print(original_env.action_space.sample())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvcll2L3afVd"
      },
      "source": [
        "#### Test the NormalizeAction wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WsCM9AUGaeBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8905a9b5-f78f-4df8-caa7-074f47f6d2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.]\n",
            "[0.48500046]\n",
            "[0.60644484]\n",
            "[0.92955446]\n",
            "[-0.25578055]\n",
            "[-0.01306045]\n",
            "[-0.536857]\n",
            "[0.8767978]\n",
            "[0.54474473]\n",
            "[-0.6915492]\n",
            "[-0.785637]\n"
          ]
        }
      ],
      "source": [
        "env = NormalizeActionWrapper(gym.make(\"Pendulum-v1\"))\n",
        "\n",
        "print(env.action_space.low)\n",
        "\n",
        "for _ in range(10):\n",
        "    print(env.action_space.sample())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Time Limits\n",
        "\n",
        "- **Termination** refers to the episode ending after reaching a terminal state that is defined as part of the environment definition.\n",
        "Examples are - task success, task failure, robot falling down etc. Notably, this also includes episodes ending in finite-horizon environments due to a time-limit\n",
        "\n",
        "- **Truncation** - Truncation refers to the episode ending after an externally defined condition (that is outside the scope of the Markov Decision Process). This could be a time-limit, a robot going out of bounds etc. This is different from time-limits in finite horizon environments as the agent in this case has no idea about this time-limit."
      ],
      "metadata": {
        "id": "IwWFpk58qtSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why the distinction between termination and truncation is important\n",
        "\n",
        "**When an episode ends due to termination we don’t bootstrap, when it ends due to truncation, we bootstrap.**\n",
        "\n"
      ],
      "metadata": {
        "id": "R5NJD1IzgCxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an illustrative example and not part of any specific algorithm:\n",
        "\n",
        "```python\n",
        "# INCORRECT\n",
        "vf_target = rew + gamma * (1 - done) * vf_next_state\n",
        "\n",
        "# CORRCET\n",
        "vf_target = rew + gamma * (1 - terminated) * vf_next_state\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Z-r2JH-ShKhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From v0.26 onwards, Gymnasium’s `env.step` API returns both termination and truncation information explicitly"
      ],
      "metadata": {
        "id": "mPaTUhvtiwgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make your own custom environment\n",
        "\n",
        "In practice this is how a gym environment looks like. Here, we have implemented a simple grid world were the agent must learn to go always left."
      ],
      "metadata": {
        "id": "ZwMx-xsaq9L5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rYzDXA9vJfz1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "class GoLeftEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment that follows gym interface.\n",
        "    This is a simple env where the agent must learn to go always left.\n",
        "    \"\"\"\n",
        "\n",
        "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "    metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "    # Define constants for clearer code\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "\n",
        "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
        "        super(GoLeftEnv, self).__init__()\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Size of the 1D-grid\n",
        "        self.grid_size = grid_size\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos = grid_size - 1\n",
        "\n",
        "        # Define action and observation space\n",
        "        # They must be gym.spaces objects\n",
        "        # Example when using discrete actions, we have two: left and right\n",
        "        n_actions = 2\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "        # The observation will be the coordinate of the agent\n",
        "        # this can be described both by Discrete and Box space\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed, options=options)\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos = self.grid_size - 1\n",
        "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == self.LEFT:\n",
        "            self.agent_pos -= 1\n",
        "        elif action == self.RIGHT:\n",
        "            self.agent_pos += 1\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Received invalid action={action} which is not part of the action space\"\n",
        "            )\n",
        "\n",
        "        # Account for the boundaries of the grid\n",
        "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "        # Are we at the left of the grid?\n",
        "        terminated = bool(self.agent_pos == 0)\n",
        "        truncated = False  # we do not limit the number of steps here\n",
        "\n",
        "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "        reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "        # Optionally we can pass additional info, we are not using that for now\n",
        "        info = {}\n",
        "\n",
        "        return (\n",
        "            np.array([self.agent_pos]).astype(np.float32),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            info,\n",
        "        )\n",
        "\n",
        "    def render(self):\n",
        "        # agent is represented as a cross, rest as a dot\n",
        "        if self.render_mode == \"console\":\n",
        "            print(\".\" * self.agent_pos, end=\"\")\n",
        "            print(\"x\", end=\"\")\n",
        "            print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine"
      },
      "source": [
        "### Validate the environment\n",
        "\n",
        "Gymnasium provides a [helper](https://gymnasium.farama.org/api/utils/#gymnasium.utils.env_checker.check_env) to check that your environment follows the Gym interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1CcUVatq-P0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a8d69e-52cc-472a-c13a-ff3dbe969ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/env_checker.py:274: UserWarning: \u001b[33mWARN: `check_env(warn=...)` parameter is now ignored.\u001b[0m\n",
            "  logger.warn(\"`check_env(warn=...)` parameter is now ignored.\")\n"
          ]
        }
      ],
      "source": [
        "from gymnasium.utils.env_checker import check_env\n",
        "env = GoLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True, skip_render_check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g"
      },
      "source": [
        "### Testing the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "i62yf2LvSAYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "330e0248-63e9-42ae-cb0f-2d4efb8536d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".........x.\n",
            "Box(0.0, 10.0, (1,), float32)\n",
            "Discrete(2)\n",
            "1\n",
            "Step 1\n",
            "obs= [8.] reward= 0 done= False\n",
            "........x..\n",
            "Step 2\n",
            "obs= [7.] reward= 0 done= False\n",
            ".......x...\n",
            "Step 3\n",
            "obs= [6.] reward= 0 done= False\n",
            "......x....\n",
            "Step 4\n",
            "obs= [5.] reward= 0 done= False\n",
            ".....x.....\n",
            "Step 5\n",
            "obs= [4.] reward= 0 done= False\n",
            "....x......\n",
            "Step 6\n",
            "obs= [3.] reward= 0 done= False\n",
            "...x.......\n",
            "Step 7\n",
            "obs= [2.] reward= 0 done= False\n",
            "..x........\n",
            "Step 8\n",
            "obs= [1.] reward= 0 done= False\n",
            ".x.........\n",
            "Step 9\n",
            "obs= [0.] reward= 1 done= True\n",
            "x..........\n",
            "Goal reached! reward= 1\n"
          ]
        }
      ],
      "source": [
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "GO_LEFT = 0\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "    print(f\"Step {step + 1}\")\n",
        "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
        "    done = terminated or truncated\n",
        "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        print(\"Goal reached!\", \"reward=\", reward)\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DhnxbqdKm1rb",
        "VIIJbSyQW9R-",
        "ZwMx-xsaq9L5"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}