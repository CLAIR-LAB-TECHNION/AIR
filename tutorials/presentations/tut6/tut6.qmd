---
title: "Value Based Methods"
subtitle: "CLAI tutorial 6"
author: "Itay Segev"
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/CLAIR_logo.png?raw=true
    css: style.css

--- 

## The Anatomy of a RL Algorithm

![The anatomy of a reinforcement learning algorithm](https://www.researchgate.net/profile/Jonathan-Boron-2/publication/344589417/figure/fig2/AS:945255583080448@1602377901230/Structure-of-a-Reinforcement-Learning-Algorithm-Source-10.png)


## A Taxonomy of RL Algorithms

![A non-exhaustive, but useful taxonomy of algorithms in modern RL](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)

## Model-Free - Policy Based


![](images/pg.png)

## Model-Free - Value Based


![](https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut06_Value_based_anatomy.png?raw=true)



## Value Functions
\

**Value Function $v^{\pi} (s)$**: 

```{=tex}
\begin{align*}
v^{\pi}(s) &\equiv \mathbb{E}^{\pi}\left[ G_t \mid S_t = s \right] \\
           &= \mathbb{E}^{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s \right]
\end{align*}
```
\
**Action-Value Function $q^{\pi}(s,a)$**: 
\

```{=tex}
\begin{align*}
q^{\pi}(s, a) &\equiv \mathbb{E}^{\pi} \left[ G_t \mid S_t = s, A_t = a \right] \\
              &= \mathbb{E}^{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a \right]
\end{align*}
```

## Bellman Equations
\
```{=tex}
\begin{align*}
v^{\pi}(s) &= \sum_{a} \pi(a|s) q^{\pi}(s, a) \\
           &= \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left[ r + \gamma v^{\pi}(s') \right]
\end{align*}
```

```{=tex}
\begin{align*}
q^{\pi}(s, a) &= \sum_{s', r} p(s', r | s, a) \left[ r + \gamma v^{\pi}(s') \right] \\
              &= \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a' | s') q^{\pi}(s', a') \right]
\end{align*}
```


## Optimal Value Functions
\

**Optimal Value Function $v^*(s)$**: 
\

```{=tex}
\begin{align*}
v^*(s) &= \max_{a} \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v^*(s') \right]
\end{align*}
```
\

**Optimal Action-Value Function $q^*(s,a)$**: 
\ 

```{=tex}
\begin{align*}
q^*(s, a) &= \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q^*(S_{t+1}, a') \mid S_t = s, A_t = a \right] \\
          &= \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \max_{a'} q^*(s', a') \right]
\end{align*}
```

## The Optimal Policy
\

-  Once we have $v^*$ , it’s easy to find the optimal policy (greedy with respect to the optimal evaluation function $v^*$.

\

-  It’s even easier if we have $q^*$ because we just take the max without even having to do the one-step lookahead (i.e using the environment’s dynamics which we often do not have)



## Value Iteration
\

![Value Iteration](https://lcalem.github.io/imgs/sutton/value_iteration.png)


## {background="#43464B" background-image="https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true"}

## Monte Carlo Control
\

![On-policy first visit MC](https://lcalem.github.io/imgs/sutton/onpolicy_first_visit_mc.png)


## Temporal Difference Learning
\

![Monte Carlo vs Temporal Difference Learning](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg)

## MC vs TD vs DP
\
\
```{=tex}
\begin{align}
v_{\pi}(s) & \doteq \mathbb{E}[G_t | S_t = s] \\
 & = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
 & = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] 
\end{align}
```


## Q-Learning
\

![Q-Learning Algorithm](https://lcalem.github.io/imgs/sutton/qlearning.png)


## {background="#43464B" background-image="https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true"}


## Sample Efficiency

How many samples do we need to get a good policy?

- Off policy: able to improve the policy without generating new samples from that policy

- On policy: each time the policy is changed, even a little bit, we need to generate new samples

![](https://braindump.jethro.dev/ox-hugo/screenshot2019-12-16_01-35-50_.png)



## Bellman Operator


The Bellman operator $\mathcal{T}$ defines the recursive relationship for value functions in MDPs.

\

**For a given policy $\pi$:**
```{=tex}
\begin{align}

(\mathcal{T}^{\pi} v)(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) \left[ r + \gamma v(s') \right]

\end{align}
```

**For the optimal value function $v^*$:**
```{=tex}
\begin{align}

(\mathcal{T} v)(s) = \max_{a} \sum_{s', r} p(s', r|s, a) \left[ r + \gamma v(s') \right]

\end{align}
```