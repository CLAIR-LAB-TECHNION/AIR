{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gymnasium\n",
    "\n",
    "Gymnasium is a project that provides an API for all single agent reinforcement learning environments. We will outline the basics of how to use Gymnasium including its four key functions: `make`, `Env.reset`, `Env.step` and `Env.render`.\n",
    "\n",
    "At the core of Gymnasium is `Env`, a high-level python class representing a markov decision process (MDP). The class provides users the ability generate an initial state, transition / move to new states given an action and the visualise the environment. Alongside `Env`, `Wrapper` are provided to help augment / modify the environment, in particular, the agent observations, rewards and actions taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install swig\n",
    "!pip install \"gymnasium[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing environments is very easy in Gymnasium and can be done via the `make` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will return an `Env` for users to interact with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classic “agent-environment loop” pictured below is simplified representation of reinforcement learning that Gymnasium implements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src='https://gymnasium.farama.org/_images/AE_loop.png' />\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical workflow\n",
    "\n",
    "First, an environment is created using `make` with an additional keyword ``\"render_mode\"`` that specifies how the environment should be visualised.\n",
    "\n",
    "After initializing the environment, we `Env.reset` the environment to get the first observation of the environment along with an additional information. For initializing the environment with a particular random seed or options (see the environment documentation for possible values) use the ``seed`` or ``options`` parameters with `reset`.\n",
    "\n",
    "As we wish to continue the agent-environment loop until the environment ends, which is in an unknown number of timesteps, we define ``episode_over`` as a variable to know when to stop interacting with the environment along with a while loop that uses it.\n",
    "\n",
    "Next, the agent performs an action in the environment, `Env.step` executes the select actions to update the environment. As a result, the agent receives a new observation from the updated environment along with a reward for taking the action. One such action-observation exchange is referred to as a **timestep**.\n",
    "\n",
    "However, after some timesteps, the environment may end, this is called the terminal state. In gymnasium, if the environment has terminated, this is returned by `step` as the third variable, ``terminated``.\n",
    "Similarly, we may also want the environment to end after a fixed number of timesteps, in this case, the environment issues a truncated signal. If either of ``terminated`` or ``truncated`` are ``True`` then we end the episode but in most cases users might wish to restart the environment, this can be done with `env.reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(80):\n",
    "    action = env.action_space.sample()  # Randomly sample an action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    img = plt.imshow(env.render())  # Render the image\n",
    "    display(plt.gcf())    # Display the image\n",
    "    clear_output(wait=True)  # Clear the output to make the animation\n",
    "    time.sleep(0.00001)  # Pause a bit before the next frame\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action and observation spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every environment specifies the format of valid actions and observations with the `action_space` and `observation_space` attributes. This is helpful for both knowing the expected input and output of the environment as all valid actions and observation should be contained with their respective space.\n",
    "\n",
    "Importantly, `Env.action_space` and `Env.observation_space` are instances of `Space`, a high-level python class that provides the key functions: `Space.contains` and `Space.sample`. Gymnasium has support for a wide range of spaces that users might need:\n",
    "\n",
    "\n",
    "\n",
    "- `Box`: describes bounded space with upper and lower limits of any n-dimensional shape.\n",
    "- `Discrete`: describes a discrete space where ``{0, 1, ..., n-1}`` are the possible values our observation or action can take.\n",
    "- `MultiBinary`: describes a binary space of any n-dimensional shape.\n",
    "- `MultiDiscrete`: consists of a series of `Discrete` action spaces with a different number of actions in each element.\n",
    "- `Text`: describes a string space with a minimum and maximum length\n",
    "- `Dict`: describes a dictionary of simpler spaces.\n",
    "- `Tuple`: describes a tuple of simple spaces.\n",
    "- `Graph`: describes a mathematical graph (network) with interlinking nodes and edges\n",
    "- `Sequence`: describes a variable length of simpler space elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1. -2.], [2. 4.], (2,), float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium.spaces import Box, Discrete, Tuple\n",
    "import numpy as np\n",
    "\n",
    "Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = Discrete(3, start=-1, seed=42)  # {-1, 0, 1}\n",
    "observation_space.sample() # Generates a single random sample from this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([-0.3991573 ,  0.21649833], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = Tuple((Discrete(2), Box(-1, 1, shape=(2,))), seed=42)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more example usage of spaces, see their [documentation](https://gymnasium.farama.org/api/spaces/) along with utility functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrappers are a convenient way to modify an existing environment without having to alter the underlying code directly.\n",
    "\n",
    "Gymnasium already provides many commonly used wrappers. Some examples:\n",
    "\n",
    "- `TimeLimit`: Issues a truncated signal if a maximum number of timesteps has been exceeded (or the base environment has issued a truncated signal).\n",
    "- `ClipAction`: Clips any action passed to ``step`` such that it lies in the base environment's action space.\n",
    "- `RescaleAction`: Applies an affine transformation to the action to linearly scale for a new low and high bound on the environment.\n",
    "- `TimeAwareObservation`: Add information about the index of timestep to observation.\n",
    "\n",
    "Most environments that are generated via `gymnasium.make` will already be wrapped by default using the `TimeLimit`, `OrderEnforcing` and `PassiveEnvChecker`.\n",
    "\n",
    "In order to wrap an environment, you must first initialize a base environment. Then you can pass this environment along with (possibly optional) parameters to the wrapper's constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.  0.  0.], 1.0, (3,), float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium.wrappers import FlattenObservation, RescaleAction, TimeAwareObservation\n",
    "base_env = gym.make(\"CarRacing-v2\")\n",
    "base_env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, 1.0, (3,), float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_env = RescaleAction(base_env, min_action=0, max_action=1)\n",
    "wrapped_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.04006759, -0.01189244,  0.04619108,  0.01103577], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env = gym.make(\"CartPole-v1\")\n",
    "base_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03463158, -0.00567901, -0.00964559, -0.01377966,  0.        ]), {})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TimeAwareObservation(base_env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03474516, -0.20066132, -0.00992119,  0.27584442,  1.        ])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 96, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v2\")\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27648,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = FlattenObservation(env)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the environment underneath the **first** wrapper by using the `gymnasium.Wrapper.env` attribute. As the `gymnasium.Wrapper` class inherits from `gymnasium.Env` then `gymnasium.Wrapper.env` can be another wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CarRacing<CarRacing-v2>>>>>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_env.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get to the environment underneath **all** of the layers of wrappers, you can use the `gymnasium.Wrapper.unwrapped` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gymnasium.envs.box2d.car_racing.CarRacing>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three common things you might want a wrapper to do:\n",
    "\n",
    "- Transform actions before applying them to the base environment\n",
    "\n",
    "- Transform observations that are returned by the base environment\n",
    "\n",
    "- Transform rewards that are returned by the base environment\n",
    "\n",
    "Such wrappers can be easily implemented by inheriting from `gymnasium.ActionWrapper`,` gymnasium.ObservationWrapper`, or `gymnasium.RewardWrapper` and implementing the respective transformation. If you need a wrapper to do more complicated tasks, you can inherit from the `gymnasium.Wrapper` class directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: normalize actions\n",
    "\n",
    "It is usually a good idea to normalize observations and actions before giving it to the agent, this prevents this [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n",
    "\n",
    "In this example, we are going to normalize the action space of *Pendulum-v1* so it lies in [-1, 1] instead of [-2, 2].\n",
    "\n",
    "Note: here we are dealing with continuous actions, hence the `gym.Box` space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Retrieve the action space\n",
    "        action_space = env.action_space\n",
    "        assert isinstance(\n",
    "            action_space, gym.spaces.Box\n",
    "        ), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
    "        # Retrieve the max/min values\n",
    "        self.low, self.high = action_space.low, action_space.high\n",
    "\n",
    "        # We modify the action space, so all actions will lie in [-1, 1]\n",
    "        env.action_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=action_space.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super(NormalizeActionWrapper, self).__init__(env)\n",
    "\n",
    "    def rescale_action(self, scaled_action):\n",
    "        \"\"\"\n",
    "        Rescale the action from [-1, 1] to [low, high]\n",
    "        (no need for symmetric action space)\n",
    "        :param scaled_action: (np.ndarray)\n",
    "        :return: (np.ndarray)\n",
    "        \"\"\"\n",
    "        return self.low + (0.5 * (scaled_action + 1.0) * (self.high - self.low))\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset the environment\n",
    "        \"\"\"\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float,bool, bool, dict) observation, reward, final state? truncated?, additional informations\n",
    "        \"\"\"\n",
    "        # Rescale action from [-1, 1] to original [low, high] interval\n",
    "        rescaled_action = self.rescale_action(action)\n",
    "        obs, reward, terminated, truncated, info = self.env.step(rescaled_action)\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test before rescaling actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.]\n",
      "[-0.4710181]\n",
      "[0.39280972]\n",
      "[-1.3493373]\n",
      "[1.1008148]\n",
      "[-0.3110773]\n",
      "[-0.7836835]\n",
      "[0.8927286]\n",
      "[-1.5097785]\n",
      "[0.11178854]\n",
      "[-0.39259312]\n"
     ]
    }
   ],
   "source": [
    "original_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "print(original_env.action_space.low)\n",
    "for _ in range(10):\n",
    "    print(original_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the NormalizeAction wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.]\n",
      "[0.9980464]\n",
      "[0.16381392]\n",
      "[0.15574437]\n",
      "[-0.99977094]\n",
      "[0.19786243]\n",
      "[0.66842103]\n",
      "[-0.92170566]\n",
      "[-0.84871]\n",
      "[0.2052033]\n",
      "[-0.8525819]\n"
     ]
    }
   ],
   "source": [
    "env = NormalizeActionWrapper(gym.make(\"Pendulum-v1\"))\n",
    "\n",
    "print(env.action_space.low)\n",
    "\n",
    "for _ in range(10):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Time Limits\n",
    "\n",
    "- **Termination** refers to the episode ending after reaching a terminal state that is defined as part of the environment definition.\n",
    "Examples are - task success, task failure, robot falling down etc. Notably, this also includes episodes ending in finite-horizon environments due to a time-limit\n",
    "\n",
    "- **Truncation** - Truncation refers to the episode ending after an externally defined condition (that is outside the scope of the Markov Decision Process). This could be a time-limit, a robot going out of bounds etc. This is different from time-limits in finite horizon environments as the agent in this case has no idea about this time-limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the distinction between termination and truncation is important\n",
    "\n",
    "**When an episode ends due to termination we don’t bootstrap, when it ends due to truncation, we bootstrap.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an illustrative example and not part of any specific algorithm:\n",
    "\n",
    "```python\n",
    "# INCORRECT\n",
    "vf_target = rew + gamma * (1 - done) * vf_next_state\n",
    "\n",
    "# CORRCET\n",
    "vf_target = rew + gamma * (1 - terminated) * vf_next_state\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From v0.26 onwards, Gymnasium’s `env.step` API returns both termination and truncation information explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your own custom environment\n",
    "\n",
    "In practice this is how a gym environment looks like. Here, we have implemented a simple grid world were the agent must learn to go always left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have two: left and right\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # The observation will be the coordinate of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            print(\".\" * self.agent_pos, end=\"\")\n",
    "            print(\"x\", end=\"\")\n",
    "            print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the environment\n",
    "\n",
    "Gymnasium provides a [helper](https://gymnasium.farama.org/api/utils/#gymnasium.utils.env_checker.check_env) to check that your environment follows the Gym interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itaysegev/miniconda3/envs/CLAI/lib/python3.12/site-packages/gymnasium/utils/env_checker.py:274: UserWarning: \u001b[33mWARN: `check_env(warn=...)` parameter is now ignored.\u001b[0m\n",
      "  logger.warn(\"`check_env(warn=...)` parameter is now ignored.\")\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.utils.env_checker import check_env\n",
    "env = GoLeftEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True, skip_render_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........x.\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Discrete(2)\n",
      "0\n",
      "Step 1\n",
      "obs= [8.] reward= 0 done= False\n",
      "........x..\n",
      "Step 2\n",
      "obs= [7.] reward= 0 done= False\n",
      ".......x...\n",
      "Step 3\n",
      "obs= [6.] reward= 0 done= False\n",
      "......x....\n",
      "Step 4\n",
      "obs= [5.] reward= 0 done= False\n",
      ".....x.....\n",
      "Step 5\n",
      "obs= [4.] reward= 0 done= False\n",
      "....x......\n",
      "Step 6\n",
      "obs= [3.] reward= 0 done= False\n",
      "...x.......\n",
      "Step 7\n",
      "obs= [2.] reward= 0 done= False\n",
      "..x........\n",
      "Step 8\n",
      "obs= [1.] reward= 0 done= False\n",
      ".x.........\n",
      "Step 9\n",
      "obs= [0.] reward= 1 done= True\n",
      "x..........\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "    done = terminated or truncated\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
