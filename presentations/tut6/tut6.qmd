---
title: "Value Based Methods"
author: "Itay Segev"
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/CLAIR_logo.png?raw=true
    css: style.css

--- 

## The Anatomy of a RL Algorithm

![The anatomy of a reinforcement learning algorithm](https://www.researchgate.net/profile/Jonathan-Boron-2/publication/344589417/figure/fig2/AS:945255583080448@1602377901230/Structure-of-a-Reinforcement-Learning-Algorithm-Source-10.png)


## A Taxonomy of RL Algorithms

![A non-exhaustive, but useful taxonomy of algorithms in modern RL](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)

## Model-Free - Policy Based


![](images/pg.png)

## Model-Free - Value Based


![](https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut06_Value_based_anatomy.png?raw=true)



## Value Functions
\

**On-Policy Value Function $V^{\pi} (s)$**: 

```{=tex}
\begin{align*}
V^{\pi}(s) = \mathbb{E}_{\tau \sim \pi}[R(\tau) \mid s_0 = s]
\end{align*}
```
\
**On-Policy Action-Value Function $Q^{\pi}(s,a)$**: 
\

```{=tex}
\begin{align*}
Q^{\pi}(s,a) = \mathbb{E}_{\tau \sim \pi}[R(\tau) \mid s_0 = s, a_0 = a]
\end{align*}
```


## Optimal Value Functions
\

**Optimal Value Function $V^*(s)$**: 
\

```{=tex}
\begin{align*}
V^*(s) = \max_{\pi} \mathbb{E}_{\tau \sim \pi}[R(\tau) \mid s_0 = s]
\end{align*}
```
\

**Optimal Action-Value Function $Q^*(s,a)$**: 
\ 

```{=tex}
\begin{align*}
Q^*(s,a) = \max_{\pi} \mathbb{E}_{\tau \sim \pi}[R(\tau) \mid s_0 = s, a_0 = a]
\end{align*}
```

## The Optimal Action

If we have $Q^*$, we can directly obtain the optimal action $a^*(s)$ as:

```{=tex}
\begin{align*}
a^*(s) = \arg \max_a Q^*(s,a)
\end{align*}
```
\

Note that there may be multiple actions which maximize $Q^*(s,a)$, in which case, all of them are optimal.


## Bellman Equations
\
```{=tex}
\begin{align*}
v^{\pi}(s) &= \sum_{a} \pi(a|s) q^{\pi}(s, a) \\
           &= \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left[ r + \gamma v^{\pi}(s') \right]
\end{align*}
```
\

```{=tex}
\begin{align*}
q^{\pi}(s, a) &= \sum_{s', r} p(s', r | s, a) \left[ r + \gamma v^{\pi}(s') \right] \\
              &= \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a' | s') q^{\pi}(s', a') \right]
\end{align*}
```


## Value Iteration
\

![Value Iteration](https://lcalem.github.io/imgs/sutton/value_iteration.png)


## {background="#43464B" background-image="https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true"}

## Monte Carlo Control
\

![On-policy first visit MC](https://lcalem.github.io/imgs/sutton/onpolicy_first_visit_mc.png)


## Temporal Difference Learning
\

![Monte Carlo vs Temporal Difference Learning](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg)

## MC vs TD vs DP
\
\
```{=tex}
\begin{align}
v_{\pi}(s) & \doteq \mathbb{E}[G_t | S_t = s] \\
 & = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
 & = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] 
\end{align}
```


## Q-Learning
\

![Q-Learning Algorithm](https://lcalem.github.io/imgs/sutton/qlearning.png)


## {background="#43464B" background-image="https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true"}


## Sample Efficiency

How many samples do we need to get a good policy?

- Off policy: able to improve the policy without generating new samples from that policy

- On policy: each time the policy is changed, even a little bit, we need to generate new samples

![](https://braindump.jethro.dev/ox-hugo/screenshot2019-12-16_01-35-50_.png)