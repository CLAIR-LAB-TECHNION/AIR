{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4576ccad",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b85040",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imitation Learning\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/1f/Makak_neonatal_imitation.png?1648499532601' width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f11d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_tiger_example.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a3a97",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_autonuoms_driving.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab2a46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_trajectories.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b5c7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_distributional_shift.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d66d12",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Data Augmentation Strategies\n",
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_nvidia_example.png?raw=true'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb4ebd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/umRdt3zGgpU?start=60&end=120\" frameborder=\"0\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Example: Drone Flying Through Forests Using Camera Data Augmentation\n",
    "# YouTubeVideo('umRdt3zGgpU', width=800, height=450)\n",
    "video_id = \"umRdt3zGgpU\"\n",
    "start_time = 60  # Start time in seconds\n",
    "end_time = 120   # End time in seconds\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}?start={start_time}&end={end_time}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\"\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a4c7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- **Diverse Data:** Including a variety of states, including mistakes and corrections, enhances the policy's robustness.\n",
    "- **Augmentation Techniques:** Leveraging domain knowledge to create additional training data can significantly improve performance.\n",
    "- **Practical Examples:** Real-world applications, such as drone navigation and robotic manipulation, demonstrate the effectiveness of these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a099f",
   "metadata": {},
   "source": [
    "## Non-Markovian Behavior\n",
    "\n",
    "Non-Markovian behavior refers to situations where the expertâ€™s actions depend on the history of observations, not just the current state.\n",
    "\n",
    "<img src=''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e7a29",
   "metadata": {},
   "source": [
    "- **Temporal Dependencies:** Recognizing and incorporating temporal dependencies is essential for tasks with non-Markovian behavior.\n",
    "- **Sequence Models:** Using models like LSTMs or Transformers can help capture these dependencies.\n",
    "- **Causal Relationships:** Avoiding causal confusion by ensuring the policy learns true causal relationships is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe8023",
   "metadata": {},
   "source": [
    "## Multimodal Behavior\n",
    "\n",
    "<img src=''>\n",
    "\n",
    "\n",
    "- **Mixture of Gaussians:**\n",
    "A simple yet effective approach is to use a mixture of Gaussians. This method involves modeling the action distribution as a combination of multiple Gaussian distributions, each representing a different mode. The neural network outputs multiple means, variances, and weights for these Gaussians, allowing it to capture the multimodal nature of the actions.\n",
    "\n",
    "- **Latent Variable Models:**\n",
    "Latent variable models introduce an additional latent variable that captures the underlying structure of the action distribution. Conditional variational autoencoders (CVAEs) are a popular choice, where the network learns to generate different modes by conditioning on this latent variable. During training, the latent variables are assigned to specific modes, helping the network distinguish between them.\n",
    "\n",
    "- **Diffusion Models:**\n",
    "Diffusion models are gaining popularity due to their effectiveness in generating complex distributions. These models start with a highly noisy version of the action and iteratively denoise it. The neural network learns to reverse the noise addition process, effectively modeling the multimodal action distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94cb8a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/w-CGSQAO5-Q?start=42&end=90\" frameborder=\"0\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_id = \"w-CGSQAO5-Q\"\n",
    "start_time = 42  # Start time in seconds\n",
    "end_time = 90   # End time in seconds\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}?start={start_time}&end={end_time}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\"\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "235ab43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/UuKAp9a6wMs?start=350&end=410\" frameborder=\"0\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_id = \"UuKAp9a6wMs\"\n",
    "start_time = 350  # Start time in seconds\n",
    "end_time = 410   # End time in seconds\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}?start={start_time}&end={end_time}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\"\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5383838",
   "metadata": {},
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_multi_tasks_learning.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c398a7d",
   "metadata": {},
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_DAgger.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4aa92",
   "metadata": {},
   "source": [
    "The limitations of imitation learning highlight the need for methods that can autonomously collect data and improve policies without extensive human involvement. In future tutorials, we will explore reinforcement learning (RL) techniques that address these challenges by allowing agents to learn from their own experiences, aiming for behaviors that surpass human performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0bd3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
