{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90cbef07",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc5199",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imitation Learning\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/1f/Makak_neonatal_imitation.png?1648499532601' width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddb8c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_tiger_example.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df5417",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_autonuoms_driving.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d82fe3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_trajectories.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f5d30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_distributional_shift.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f662e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Augmentation Strategies\n",
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_nvidia_example.png?raw=true'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edd783f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#@title Example: Drone Flying Through Forests Using Camera Data Augmentation\n",
    "# YouTubeVideo('umRdt3zGgpU', width=800, height=450)\n",
    "video_id = \"umRdt3zGgpU\"\n",
    "start_time = 104  # Start time in seconds\n",
    "end_time = 144   # End time in seconds\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}?start={start_time}&end={end_time}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723868ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/umRdt3zGgpU?start=104&end=144\" frameborder=\"0\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175e5e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "- **Diverse Data:** Including a variety of states, including mistakes and corrections, enhances the policy's robustness.\n",
    "- **Augmentation Techniques:** Leveraging domain knowledge to create additional training data can significantly improve performance.\n",
    "- **Practical Examples:** Real-world applications, such as drone navigation and robotic manipulation, demonstrate the effectiveness of these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816427cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-Markovian Behavior\n",
    "\n",
    "Non-Markovian behavior refers to situations where the expertâ€™s actions depend on the history of observations, not just the current state.\n",
    "\n",
    "- **Temporal Dependencies:** Recognizing and incorporating temporal dependencies is essential for tasks with non-Markovian behavior.\n",
    "- **Sequence Models:** Using models like LSTMs or Transformers can help capture these dependencies.\n",
    "- **Causal Relationships:** Avoiding causal confusion by ensuring the policy learns true causal relationships is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd26cefd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "video_id = \"UuKAp9a6wMs\"\n",
    "start_time = 350  # Start time in seconds\n",
    "end_time = 410   # End time in seconds\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}?start={start_time}&end={end_time}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab33b9c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/UuKAp9a6wMs?start=350&end=410\" frameborder=\"0\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4b809",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multimodal Behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1d267",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Mixture of Gaussians:**\n",
    "This method involves modeling the action distribution as a combination of multiple Gaussian distributions, each representing a different mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422fad5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Latent Variable Models:**\n",
    "Latent variable models introduce an additional latent variable that captures the underlying structure of the action distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19745665",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Diffusion Models:**\n",
    "Diffusion models are gaining popularity due to their effectiveness in generating complex distributions. These models start with a highly noisy version of the action and iteratively denoise it. The neural network learns to reverse the noise addition process, effectively modeling the multimodal action distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dce6b749",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "video_id = \"w-CGSQAO5-Q\"\n",
    "start_time = 42  # Start time in seconds\n",
    "end_time = 90   # End time in seconds\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}?start={start_time}&end={end_time}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c56aac97",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/w-CGSQAO5-Q?start=42&end=90\" frameborder=\"0\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960025c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_multi_tasks_learning.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909bc14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_DAgger.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd68d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The limitations of imitation learning highlight the need for methods that can autonomously collect data and improve policies without extensive human involvement. In future tutorials, we will explore reinforcement learning (RL) techniques that address these challenges by allowing agents to learn from their own experiences, aiming for behaviors that surpass human performance."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
